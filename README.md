# Second-order-methods-in-Training-Neural-Networks
Term paper done as part of "Optimization Methods" course, IIIT Hyderabad 

## Description
This work provides an overview of second-order optimization methods for neural
network training. It goes into the fundamentals of the New-
tonâ€™s methods, quasi-Newton, Gauss-Newton, and Levenberg-
Marquardt, Approximate Greatest Descent and Hessian-Free
optimization. We perform several experiments and compare first
order method(Gradient Descent) with two second order meth-
ods(Levenberg Marquardt, Gauss Newton) on various parame-
ters and analyse the viability and performance of optimization
approaches.

* [video presentation](https://youtu.be/ij5Gpg131n4)
* [experimentation code](https://colab.research.google.com/drive/1sHeR5j1R_gd8vVBN_Mm51R201fOVdWLx?usp=sharing)

